{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instanseg.utils.data_download import create_raw_datasets_dir, create_processed_datasets_dir, download_and_extract\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import fastremap\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from instanseg.utils.utils import show_images, _move_channel_axis\n",
    "#aws s3 cp --no-sign-request s3://monkey-training/ ./ --recursive\n",
    "monkey_dir = Path(\"../Raw_Datasets/Monkey\")\n",
    "\n",
    "files = sorted(os.listdir(os.path.join(monkey_dir ,\"annotations\",\"xml\")))\n",
    "\n",
    "label_ids = []\n",
    "means_list = []\n",
    "annotations_dict = {}\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "for file in tqdm(files):\n",
    "\n",
    "    split = np.random.choice([\"train\", \"val\"], p=[0.8, 0.2])\n",
    " \n",
    "    img_pascpg_path = Path(monkey_dir) / (\"images/pas-cpg/\" + file.split(\".\")[0] + \"_PAS_CPG.tif\")\n",
    "    img_pasdiagnostic_path = Path(monkey_dir) / (\"images/pas-diagnostic/\" + file.split(\".\")[0] + \"_PAS_Diagnostic.tif\")\n",
    "   # img_pasoriginal_path = Path(monkey_dir) / (\"images/pas-original/\" + file.split(\".\")[0] + \"_PAS_Original.tif\")\n",
    "    ihc_path = Path(monkey_dir) / (\"images/ihc/\" + file.split(\".\")[0] + \"_IHC_CPG.tif\")\n",
    "    \n",
    "    from tiffslide import TiffSlide\n",
    "    slidepascpg = TiffSlide(img_pascpg_path)\n",
    "    slideihc = TiffSlide(ihc_path)\n",
    "\n",
    "\n",
    "    tree = ET.parse(monkey_dir/(\"annotations/xml/\"+file))\n",
    "    root = tree.getroot()  # Get the root of the XML\n",
    "\n",
    "    # if split == \"val\":\n",
    "    #     destination_img = \"/home/cdt/Documents/Projects/monkey-challenge-instanseg/evaluation/validation_set/images/kidney-transplant-biopsy-wsi-pas/\"\n",
    "    #     destination_mask = \"/home/cdt/Documents/Projects/monkey-challenge-instanseg/evaluation/validation_set/images/tissue-mask/\"\n",
    "        \n",
    "    #     #move images to inference folder\n",
    "    #     import shutil\n",
    "    #     shutil.copy(monkey_dir / (\"images/pas-cpg/\" + file.split(\".\")[0] + \"_PAS_CPG.tif\"), destination_img)\n",
    "    #     shutil.copy(monkey_dir / (\"images/tissue-masks/\" + file.split(\".\")[0] + \"_mask.tif\"), destination_mask)\n",
    "        \n",
    "    #     shutil.copy(monkey_dir / (\"annotations/json/\" + file.split(\".\")[0] + \"_inflammatory-cells.json\"), \n",
    "    #     '/home/cdt/Documents/Projects/monkey-challenge-instanseg/evaluation/ground_truth')\n",
    "\n",
    "    #     shutil.copy(monkey_dir / (\"annotations/json/\" + file.split(\".\")[0] + \"_lymphocytes.json\"), \n",
    "    #     '/home/cdt/Documents/Projects/monkey-challenge-instanseg/evaluation/ground_truth')\n",
    "\n",
    "    #     shutil.copy(monkey_dir / (\"annotations/json/\" + file.split(\".\")[0] + \"_monocytes.json\"), \n",
    "    #     '/home/cdt/Documents/Projects/monkey-challenge-instanseg/evaluation/ground_truth')\n",
    "\n",
    "    coords = []\n",
    "\n",
    "    annotations_dict[file] = []\n",
    "\n",
    "    # Iterate over each annotation and extract relevant information\n",
    "    for annotation in root.findall('.//Annotation'):\n",
    "        name = annotation.get('Name')\n",
    "        part_of_group = annotation.get('PartOfGroup')\n",
    "        _type = annotation.get('Type')\n",
    "      \n",
    "        if _type == \"Polygon\":\n",
    "            coords_ROI = []\n",
    "            for coordinate in annotation.findall('.//Coordinate'):\n",
    "                x = float(coordinate.get('X'))\n",
    "                y = float(coordinate.get('Y'))\n",
    "                coords_ROI.append([x, y])\n",
    "\n",
    "            coords_ROI = np.array(coords_ROI)\n",
    "\n",
    "            x_min, y_min = coords_ROI.min(axis=0)\n",
    "            x_max, y_max = coords_ROI.max(axis=0)\n",
    "            bbox_width = int(x_max - x_min)\n",
    "            bbox_height = int(y_max - y_min)\n",
    "\n",
    "            # Read the bounding box from the slide\n",
    "            rgb_data = slidepascpg.read_region(\n",
    "                (int(x_min), int(y_min)),\n",
    "                0,\n",
    "                (bbox_width, bbox_height),\n",
    "                as_array=True,\n",
    "            )\n",
    "\n",
    "            ihc_data = slideihc.read_region(\n",
    "                (int(x_min), int(y_min)),\n",
    "                0,\n",
    "                (bbox_width, bbox_height),\n",
    "                as_array=True,\n",
    "            )\n",
    "\n",
    "\n",
    "            mask = Image.new(\"L\", (bbox_width, bbox_height), 0)\n",
    "            polygon = coords_ROI - [x_min, y_min]  # Translate polygon to local bbox coordinates\n",
    "            ImageDraw.Draw(mask).polygon(polygon.flatten().tolist(), outline=1, fill=1)\n",
    "            # Convert the mask to a NumPy array\n",
    "            binary_mask = np.array(mask)\n",
    "\n",
    "            annotations_dict[file].append({ \"split\": split,\n",
    "                                            \"pas-cpg\":rgb_data,\n",
    "                                            \"ihc\":ihc_data,\n",
    "                                            \"polygon\": coords_ROI, \n",
    "                                            \"mask\": binary_mask, \n",
    "                                            \"bbox\" : [x_min, y_min, x_max, y_max], \n",
    "                                            \"dots\" : []})\n",
    "\n",
    "            #show_images(rgb_data)\n",
    "\n",
    "    for annotation in root.findall('.//Annotation'):\n",
    "        name = annotation.get('Name')\n",
    "        part_of_group = annotation.get('PartOfGroup')\n",
    "        _type = annotation.get('Type')\n",
    "        \n",
    "        if _type == \"Dot\":\n",
    "            # Find the coordinates\n",
    "            coordinates = annotation.find('.//Coordinate')\n",
    "            x = int(float(coordinates.get('X')))\n",
    "            y = int(float(coordinates.get('Y')))\n",
    "            c = 0 if part_of_group == \"lymphocytes\" else 1\n",
    "\n",
    "            for i,annotation in enumerate(annotations_dict[file]):\n",
    "                if annotation[\"bbox\"][0] < x < annotation[\"bbox\"][2] and annotation[\"bbox\"][1] < y < annotation[\"bbox\"][3]:\n",
    "                    annotations_dict[file][i][\"dots\"].append([y - annotation[\"bbox\"][1] ,x - annotation[\"bbox\"][0],c])\n",
    "\n",
    "    \n",
    "                        \n",
    "                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leukocytes_dots = 0\n",
    "detected_leukocytes = 0\n",
    "\n",
    "import os\n",
    "from instanseg.utils.pytorch_utils import get_masked_patches\n",
    "from instanseg.instanseg import _to_tensor_float32, _rescale_to_pixel_size\n",
    "import torchstain\n",
    "from instanseg import InstanSeg\n",
    "\n",
    "os.environ[\"INSTANSEG_BIOIMAGEIO_PATH\"] = '/home/cdt/Documents/Projects/InstanSeg/instanseg_thibaut/instanseg/bioimageio_models/'\n",
    "os.environ['INSTANSEG_DATASET_PATH'] = \"../datasets/\"\n",
    "\n",
    "instanseg_script = torch.jit.load(\"instanseg/instanseg_brightfield_monkey.pt\") #download for github release\n",
    "brightfield_nuclei = InstanSeg(instanseg_script, verbosity = 0)\n",
    "\n",
    "patch_size = 128\n",
    "destination_pixel_size = 0.5 # 2420\n",
    "rescale_output = False if destination_pixel_size == 0.5 else True\n",
    "\n",
    "image_types  = [\"cpg\"]#, \"ihc\"]\n",
    "\n",
    "for image_type in image_types:\n",
    "\n",
    "  if image_type == \"cpg\":\n",
    "    image_key  = \"pas-cpg\"\n",
    "  else:\n",
    "    image_key = \"ihc\"\n",
    "\n",
    "  device = \"cpu\"\n",
    "\n",
    "  np.random.seed(0)\n",
    "  import h5py\n",
    "  with h5py.File(Path(os.environ['INSTANSEG_DATASET_PATH']) / f\"monkey_{image_type}_gold.h5\", \"w\") as f:\n",
    "\n",
    "      f.attrs['class_names'] = str({\"0\": \"lymphocytes\", \"1\": \"monocytes\", \"2\" : \"other\"})  # Convert to string since HDF5 attributes must be simple types\n",
    "      f.attrs['pixel_size'] = destination_pixel_size\n",
    "\n",
    "      for split in ['train', 'val']:\n",
    "          f.create_dataset(f\"{split}/data\", shape=(0, 4, patch_size, patch_size),\n",
    "          dtype=np.uint8, maxshape=(None, 4, patch_size, patch_size),\n",
    "          chunks=(1, 4, patch_size, patch_size),)\n",
    "          f.create_dataset(f\"{split}/labels\", shape=(0, 1), dtype=np.uint8, maxshape=(None, 1))\n",
    "\n",
    "\n",
    "      for file in tqdm(annotations_dict.keys()):\n",
    "          split = annotations_dict[file][0][\"split\"]\n",
    "\n",
    "          for annotation in annotations_dict[file]:\n",
    "\n",
    "              array = _to_tensor_float32(annotation[\"pas-cpg\"])\n",
    "\n",
    "              labels , input_tensor = brightfield_nuclei.eval_medium_image(array,\n",
    "              pixel_size = 0.2420, rescale_output = rescale_output, seed_threshold = 0.05, tile_size= 1024)\n",
    "\n",
    "              dots = torch.tensor(annotation[\"dots\"]).to(device)\n",
    "              dots[:,:2] = dots[:,:2] * 0.2420 / destination_pixel_size\n",
    "\n",
    "              mask = _rescale_to_pixel_size(_to_tensor_float32(annotation[\"mask\"]), 0.2420, destination_pixel_size).to(device)\n",
    "              \n",
    "              labels = labels.to(device) * torch.tensor(mask).bool()\n",
    "              canvas = torch.zeros_like(labels)\n",
    "              dots = torch.tensor(dots, dtype=torch.long)\n",
    "              canvas[:,:,dots[:,0],dots[:,1]] = dots[:,2].float() + 1\n",
    "              monocytes = labels * torch.isin(labels,labels * (canvas == 2).float()).float()\n",
    "              lymphocytes = labels * torch.isin(labels,labels * (canvas == 1).float()).float()\n",
    "              other_cells = (labels * ~torch.isin(labels,labels * (canvas > 0).float())).float()\n",
    "\n",
    "              img_tensor = _rescale_to_pixel_size(_to_tensor_float32(annotation[image_key]), 0.2420, destination_pixel_size).byte().to(device)\n",
    "\n",
    "              assert img_tensor.shape[-2:] == labels.shape[-2:]\n",
    "              detected_leukocytes += len(torch.unique(monocytes + lymphocytes)) - 1\n",
    "              leukocytes_dots += len(dots)\n",
    "\n",
    "\n",
    "              if len(torch.unique(monocytes)) > 1:\n",
    "                crops,masks = get_masked_patches(monocytes,img_tensor, patch_size=patch_size)\n",
    "                crops = (crops).to(torch.uint8)\n",
    "                masks = (masks).to(torch.uint8)\n",
    "                x_monocytes =(torch.cat((crops,masks),dim= 1))\n",
    "                y_monocytes = torch.zeros(len(x_monocytes),dtype = torch.long) + 1\n",
    "              else:\n",
    "                x_monocytes = torch.zeros(0,4,patch_size,patch_size).to(device)\n",
    "                y_monocytes = torch.zeros(0,dtype = torch.long) + 1\n",
    "\n",
    "\n",
    "              if len(torch.unique(lymphocytes)) > 1:\n",
    "                crops,masks = get_masked_patches(lymphocytes,img_tensor, patch_size=patch_size)\n",
    "                crops = (crops).to(torch.uint8)\n",
    "                masks = (masks).to(torch.uint8)\n",
    "                x_lymphocytes =(torch.cat((crops,masks),dim= 1))\n",
    "                y_lymphocytes = torch.zeros(len(x_lymphocytes),dtype = torch.long) + 0\n",
    "              else:\n",
    "                x_lymphocytes = torch.zeros(0,4,patch_size,patch_size).to(device)\n",
    "                y_lymphocytes = torch.zeros(0,dtype = torch.long) + 0\n",
    "\n",
    "              if len(torch.unique(other_cells)) > 1:\n",
    "                crops,masks = get_masked_patches(other_cells,img_tensor, patch_size=patch_size)\n",
    "                crops = (crops).to(torch.uint8)\n",
    "                masks = (masks).to(torch.uint8)\n",
    "                x_other =(torch.cat((crops,masks),dim= 1))\n",
    "                y_other = torch.zeros(len(x_other),dtype = torch.long) + 2\n",
    "              else:\n",
    "                x_other = torch.zeros(0,4,patch_size,patch_size).to(device)\n",
    "                y_other = torch.zeros(0,dtype = torch.long) + 2\n",
    "\n",
    "              x = torch.cat((x_monocytes,x_lymphocytes,x_other),dim = 0)\n",
    "              y = torch.cat((y_monocytes,y_lymphocytes,y_other),dim = 0).numpy()[:,None]\n",
    "\n",
    "              if len(x) != len(y):\n",
    "                    pdb.set_trace()\n",
    "\n",
    "              data_ds = f[f\"{split}/data\"]\n",
    "              labels_ds = f[f\"{split}/labels\"]\n",
    "\n",
    "              data_ds.resize((data_ds.shape[0] + x.shape[0],) + x.shape[1:])\n",
    "              data_ds[-x.shape[0]:, ...] = (x).cpu().numpy().astype(np.uint8)\n",
    "              labels_ds.resize((labels_ds.shape[0] + y.shape[0],) + y.shape[1:])\n",
    "              labels_ds[-y.shape[0]:, ...] = y.astype(np.uint8)\n",
    "\n",
    "          \n",
    "\n",
    "  undetected_percent = ( leukocytes_dots - detected_leukocytes) / leukocytes_dots\n",
    "  print(f\"Detected {detected_leukocytes} out of {leukocytes_dots} dots. { 100 - undetected_percent * 100:.2f}% detected\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO CONTINUE, YOU NEED TO TRAIN A MODEL ON THE IHC IMAGES (see Readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import fastremap\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from instanseg.utils.utils import show_images, _move_channel_axis\n",
    "from tiling import get_random_non_empty_tiles\n",
    "from tiffslide import TiffSlide\n",
    "\n",
    "\n",
    "os.environ['INSTANSEG_DATASET_PATH'] = \"../datasets/\"\n",
    "\n",
    "from instanseg.instanseg import InstanSeg, _rescale_to_pixel_size, _to_tensor_float32, to_ndim\n",
    "from instanseg.utils.pytorch_utils import get_masked_patches\n",
    "\n",
    "instanseg_script = torch.jit.load(\"instanseg/instanseg_brightfield_monkey.pt\")\n",
    "brightfield_nuclei = InstanSeg(instanseg_script, verbosity = 0)\n",
    "\n",
    "import os\n",
    "os.environ[\"INSTANSEG_OUTPUT_PATH\"] = \"../outputs/\"\n",
    "from utils import get_classifier\n",
    "classifier = get_classifier(\"1922985\").to(\"cuda\").eval() #THIS IS THE IHC CLASSIFIER. THERE IS A COPY ON GITHUB RELEASES\n",
    "\n",
    "import ttach as tta\n",
    "transforms = tta.Compose(\n",
    "    [\n",
    "        tta.HorizontalFlip(),\n",
    "        tta.Rotate90(angles=[0, 180]),  \n",
    "    ]\n",
    ")\n",
    "tta_classifier = tta.ClassificationTTAWrapper(classifier, transforms, merge_mode='mean').eval()\n",
    "\n",
    "\n",
    "patch_size = 128\n",
    "destination_pixel_size = 0.5\n",
    "normalise = True\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "monkey_dir = Path(\"../Raw_Datasets/Monkey\")\n",
    "files = os.listdir(os.path.join(monkey_dir ,\"annotations\",\"xml\"))\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "import h5py\n",
    "with h5py.File(Path(os.environ['INSTANSEG_DATASET_PATH']) / \"monkey_cpg_silver.h5\", \"w\") as f:\n",
    "\n",
    "    f.attrs['class_names'] = str({\"0\": \"lymphocytes\", \"1\": \"monocytes\", \"2\" : \"other\"})  # Convert to string since HDF5 attributes must be simple types\n",
    "    f.attrs['pixel_size'] = destination_pixel_size\n",
    "\n",
    "    for split in ['train', 'val']:\n",
    "        f.create_dataset(f\"{split}/data\", shape=(0, 4, patch_size, patch_size), \n",
    "                        dtype=np.uint8, maxshape=(None, 4, patch_size, patch_size), \n",
    "                        chunks=(1, 4, patch_size, patch_size),\n",
    "                      #  compression = \"lzf\",\n",
    "        )\n",
    "        f.create_dataset(f\"{split}/labels\", shape=(0, 1), dtype=np.uint8, maxshape=(None, 1))\n",
    "\n",
    "\n",
    "    for file in tqdm(files):\n",
    "\n",
    "        split = annotations_dict[file][0][\"split\"]\n",
    "\n",
    "        img_pascpg_path = Path(monkey_dir) / (\"images/pas-cpg/\" + file.split(\".\")[0] + \"_PAS_CPG.tif\")\n",
    "        ihc_path = Path(monkey_dir) / (\"images/ihc/\" + file.split(\".\")[0] + \"_IHC_CPG.tif\")\n",
    "        \n",
    "        slidepascpg = TiffSlide(img_pascpg_path)\n",
    "        slideihc = TiffSlide(ihc_path)\n",
    "\n",
    "        tiles_he,tiles_ihc = get_random_non_empty_tiles(slidepascpg,slideihc, num_images=1000, tile_size=1024) #400\n",
    "\n",
    "\n",
    "        for tile_he,tile_ihc in zip(tiles_he,tiles_ihc):\n",
    "\n",
    "           # show_images(tile_he,tile_ihc,labels)\n",
    "       \n",
    "            labels , input_tensor = brightfield_nuclei.eval_small_image(tile_he,\n",
    "            pixel_size = 0.2420, rescale_output = False, seed_threshold = 0.05)\n",
    "\n",
    "            ihc_tensor = _rescale_to_pixel_size(_to_tensor_float32(tile_ihc), 0.2420, destination_pixel_size).byte().to(device)\n",
    "\n",
    "            he_tensor = _rescale_to_pixel_size(_to_tensor_float32(tile_he), 0.2420, destination_pixel_size).byte().to(device)\n",
    "\n",
    "            if labels.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            assert ihc_tensor.shape[-2:] == he_tensor.shape[-2:]\n",
    "            assert ihc_tensor.shape[-2:] == labels.shape[-2:]\n",
    "\n",
    "            crops,masks = get_masked_patches(labels.to(device),ihc_tensor, patch_size=patch_size)\n",
    "            crops = (crops) / 255\n",
    "            masks = (masks)\n",
    "            x_ihc =(torch.cat((crops,masks),dim= 1))\n",
    "\n",
    "            crops,masks = get_masked_patches(labels.to(device),he_tensor, patch_size=patch_size)\n",
    "            crops = (crops).to(torch.uint8)\n",
    "            masks = (masks).to(torch.uint8)\n",
    "            x =(torch.cat((crops,masks),dim= 1)).cpu().numpy().astype(np.uint8)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_size = 128\n",
    "               # y_hat_he = torch.cat([classifier_he.forward(x[i:i+batch_size].float().to(\"cuda\")) for i in range(0,len(x_ihc),batch_size)],dim = 0)\n",
    "               # y_hat_he = y_hat_he.argmax(dim = 1).cpu()\n",
    "\n",
    "                y_hat= torch.cat([tta_classifier.forward(x_ihc[i:i+batch_size].float().to(\"cuda\")) for i in range(0,len(x_ihc),batch_size)],dim = 0)\n",
    "                y_hat = y_hat.argmax(dim = 1).cpu()\n",
    "\n",
    "          \n",
    "            # show_images(*x_ihc[y_hat == 1][:8,:3],n_cols = 8)\n",
    "            # show_images(*x_ihc[y_hat == 0][:8,:3],n_cols = 8)\n",
    "\n",
    "            y = y_hat.numpy()[:,None]\n",
    "\n",
    "            unique, counts = np.unique(y, return_counts=True)\n",
    "            min_count = counts.min()\n",
    "            y_subset = np.concatenate([y[y == i][:min_count + 10] for i in range(3)])\n",
    "            x_subset = np.concatenate([x[(y == i).squeeze()][:min_count + 10] for i in range(3)])\n",
    "\n",
    "\n",
    "            if x_subset.ndim == 5:\n",
    "                x_subset = x_subset[0]\n",
    "            x = x_subset\n",
    "            y = y_subset[:,None]\n",
    "\n",
    "            data_ds = f[f\"{split}/data\"]\n",
    "            labels_ds = f[f\"{split}/labels\"]\n",
    "\n",
    "            data_ds.resize((data_ds.shape[0] + x.shape[0],) + x.shape[1:])\n",
    "            data_ds[-x.shape[0]:, ...] = x\n",
    "            labels_ds.resize((labels_ds.shape[0] + y.shape[0],) + y.shape[1:])\n",
    "            labels_ds[-y.shape[0]:, ...] = y.astype(np.uint8)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "os.environ[\"INSTANSEG_OUTPUT_PATH\"] = \"../outputs\"\n",
    "from utils import get_classifier\n",
    "\n",
    "model = \"1937330\" \n",
    "\n",
    "classifier = get_classifier(model).to(\"cpu\").eval()\n",
    "torch.jit.save(torch.jit.script(classifier.path_classifier.eval()), f\"{model}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
